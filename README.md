## Overview

EdgeTB is a hybrid testbed for distributed machine learning at the edge. It allows using Docker containers and physical
nodes to create hybrid test environments. On the one hand, the existence of physical nodes improves the computing
fidelity and network fidelity of EdgeTB, making it close to the physical testbed. On the other hand, compared with the
physical testbed, the adoption of emulators makes it easier for EdgeTB to generate large-scale and network-flexible test
environments.

## Installation

1. At least 2 computing devices, one acts as Controller, and others act as Workers (as physical nodes, class:
   `PhysicalNode` or as emulator, class: `Emulator`).
2. | |Requirement|
   | --- | --- |
   |Controller|python3, python3-pip, NFS-Server|
   |Worker (PhysicalNode)|python3, python3-pip, NFS-Client, iproute (iproute2)|
   |Worker (Emulator)|python3, python3-pip, NFS-Client, iproute (iproute2), Docker|
3. Copy ```controller``` into Controller and install the python packages defined in ```controller/ctl_req.txt```.
4. Copy ```worker``` into Worker and install the python packages defined in ```worker/agent_req.txt```.

## File structure

```
controller
   ├─ dml_app  >>  Where we prepare roles, static files, shared by NFS
      ├─ dml_req.txt  >>  Role's execution environment
      ├─ Dockerfile  >>  Role's execution environment
      ├─ role_base.py  >>  Role's functions, base class
      ├─ gl_peer_new.py  >>  Role's functions, an example, inherited from role_base.Role
      ├─ nns  >>  Neural networks
      ├─ dml_utils.py
      └─ worker_utils.py
   ├─ dml_file  >>  Dynamically generated files for each node, transmitted over the network
      ├─ conf  >>  Generated by dml_tool/*_conf.py before running test, send to each node
      └─ log  >>  Received from each node
   ├─ dataset  >>  Splitted dataset, static files, shared by NFS
   ├─ dml_tool
      ├─ gl_dataset.json  >>  Dataset definition of all Gossip peer nodes, an example
      ├─ dataset_conf.py  >>  Used to generate dataset conf file for each node
      ├─ gl_structure.json  >>  Structure definition of all Gossip peer nodes, an example
      ├─ gl_structure_conf_new.py  >>  Used to generate structure conf file for each Gossip peer node
      ├─ splitter_fashion_mnist.py  >>  Used to split dataset
      └─ splitter_utils.py
   ├─ controller_base.py  >>  Controller, base class
   ├─ manager_base.py  >>  Runtime manager, base class, works with Controller
   ├─ appConfig-gl.yml  >>  Test environment definition, an example
   ├─ gl_run_new.py  >>  Customized runtime manager and controller for Gossip Learning, as well as entrypoint
   └─ links.json  >>  Network links definition
   
worker
   ├─ agent.py  >>  Used to communicate with controller/*_run.py
   ├─ dml_app  >>  mount point of controller/dml_app, over NFS
   ├─ dml_file
      ├─ conf  >>  Received from controller
      └─ log  >>  Generated by each node while running test, send to controller
   └─ dataset  >>  mount point of controller/dataset, over NFS
```

## Usage

### Workflow overview

Prepare roles, neural networks, dataset >> Define test environment >> Run it >> Collect result.

### Run an example

Here we take Gossip Learning for example and take you through the steps of running DML on your own environment.

1. The only thing to do in Worker is to run `worker_agent.py` with python3 __with root privileges__. We need to mount
NFS and install Python packages via `python3-pip` which require root privileges.
2. All the following operations should be completed in the Controller (You may also need to have a look at Worker
outputs to see if a sudo password is required. If so, enter your password in Worker.)
3. Modify `controller/appConfig-gl.yml` to define your test environment. See `controller/appConfig-template.yml` for
instructions.
4. Modify `controller/dml_tool/gl_dataset.json` to define the data used by each node.
5. Modify `controller/dml_tool/gl_structure.json` to define the DML structure of each node. See
`controller/dml_tool/README.md` for more.
6. Run `controller/gl_run_new.py` with python3 __with root privileges__ and keep it running on a terminal (called
Term).
7. In path `controller/dml_tool`:
    1. Run command `python3 dataset_conf.py -f gl_dataset.json` to generate dataset conf files;
    2. Run command `python3 conf_generator.py -f gl_structure.json -p 4444 gl_structure_conf_new.GlConfGenerator` to
    generate DML structure conf files. (`4444` corresponds to the dmlPort in appConfig file.)
8. Run command `curl localhost:3333/conf?type=1` to send those dataset conf files to each node. Wait until all nodes
have received the dataset conf file.
9. Run command `curl localhost:3333/conf?type=2` to send those DML structure files to each node. Wait until all nodes
have received the structure conf file.
10. When a `tc finish` is displayed on the Term, you are ready to move on.
11. Run command `curl localhost:3333/start` to start all nodes.
12. When there is no node _Gossip_, run command `curl localhost:3333/finish` to stop all nodes and collect result
files.
